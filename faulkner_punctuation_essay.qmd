---
title: "Faulkner and punctuation"
author: "Johannes Burgers"
format: html
editor: visual
---

## Precursors

```{r packages}
library(tidyverse)
library(tidytext)
library(qdapRegex )
library(psych)
library(EnvStats)
library(ggpubr)
library(isotree)
library(broom)
library(AICcmodavg)
library(cluster)    # clustering algorithms
library(factoextra)
library(robustbase)
library(univOutl)
```

## Introduction

There is a lot to say about Faulkner and punctuation.

In this pieces I will argue:

Faulkner pushes the very definition of a sentence in order to demonstrate the continuity of time. Yet, how he experiments with punctuation to achieve this end changes throughout his career. What's more, the philosophy of the long sentence appears to only have salience for some of his fiction and not all of it.

## Critical literature on Faulkner

## Methodology and Editing Faulkner

### Corpus Description

Faulkner's corpus is quite heterogeneous, and consists of novels, short stories, poetry, film scripts, and a text that hovers between a stage play and a novel: *Requiem for a Nun*. Even within these broad categories there are further subdivisions. Speaking of the novels and short-stories more narrowly, there are the 14 novels and 54 short-stories that take place in his famous Yoknapatawpha County. Among these are "uncollected" short stories that were only ever published in magazines and then incorporated wholesale or in part into novels such as *The Unvanquished*, *The Hamlet*, and *Go Down, Moses*. There is also a further set of unpublished stories that are interesting in their own right or that have resonances with Faulkner's more well-known published texts. This leads to a rich body of work that makes for a poor experimental design. Using computational techniques to analyze texts, works best when detecting patterns and exceptions within a particular domain. When a corpus is heterogeneous with many unique texts, it is harder to detect the rules when everything appears to be an exception. While it is possible to make certain generalizations about Faulkner's writing, these observations across the entire corpus are not very meaningful. For example, Faulkner writes longer sentences than other authors `r round(mean(summary_punctuation$string_length),0)`, but this number varies quite widely from an entirely average x words per sentence to works that are exceptionally verbose. Therefore, it makes more sense to stay away from large assumptions about how Faulkner punctuated his texts, and instead focus on how Faulkner did so in a particular context, be it his novels, short stories, or other writings. These local insights can then be stitched together to create a more global collage.

## Computation

### Preprocess Data

```{r load_data, eval=FALSE, include=FALSE}
#| include: false

all_works <-
  list.files(file.path("data"), full.names = TRUE, pattern = "*.txt") %>% #grab a list of all the files with .txt extension
  #the full.names value needs to set to TRUE to get the full path. For some reason you will get a "permission denied" error if you do not do this.
  map_df(~ tibble(  #the map function performs the same command on all parts of the data set. In this case the .txt files
    text = read_file(.), #read the files
    date = ifelse(
      str_detect(basename(.), "[:digit:]{4}") == TRUE,
      str_extract(basename(.), "[:digit:]{4}"),
      NA), #see if there is a date in yyyy format, i.e. 1987, and extract the date, if it can't find date NA
    title=str_extract(basename(.), "(?<=_)[:alpha:]*"),
    code=str_extract(basename(.), "[:upper:]+"),
    revised = str_detect(basename(.),"_revised")
  )) 

```

```{r eval=FALSE, include=FALSE}
secondary_works <- list.files(file.path("secondary_authors"), full.names = TRUE, pattern = "*.txt") %>% #grab a list of all the files with .txt extension
  #the full.names value needs to set to TRUE to get the full path. For some reason you will get a "permission denied" error if you do not do this.
  map_df(~ tibble(  #the map function performs the same command on all parts of the data set. In this case the .txt files
    text = read_file(.), #read the files
    date = ifelse(
      str_detect(basename(.), "[:digit:]{4}") == TRUE,
      str_extract(basename(.), "[:digit:]{4}"),
      NA), #see if there is a date in yyyy format, i.e. 1987, and extract the date, if it can't find date NA
    title=str_extract(basename(.), "(?<=_)[:alpha:]*"),
    code=str_extract(basename(.), "[:upper:]+"),
    revised = str_detect(basename(.),"_revised")
  )) 
```

```{r normalize_punctuation, eval=FALSE, include=FALSE}
#| include: false

#This helps normalize how quote marks are used in the text.

all_works_clean <- all_works %>%
  mutate(text = gsub("[,\\!\\?\\.](?=[^’‘]*’[\\s—-])", " ", text, perl =
                       TRUE))
```

```{r eval=FALSE, include=FALSE}
secondary_works_clean <- secondary_works %>%
  mutate(text = gsub("[,\\!\\?\\.](?=[^’‘]*’[\\s—-])", " ", text, perl =
                       TRUE))
```

### Import Data

```{r export_CSV, eval=FALSE, include=FALSE}
#| include: false

#saved version of the preprocessing to help improve spead on repeat runs.

write_csv(all_works_clean, "processed_data/all_works_clean.csv")
```

```{r import_CSV}
#import saved version of text.

all_works_clean_import <- read_csv("processed_data/all_works_clean.csv", show_col_types = FALSE)
```

### Secondary Cleaning

```{r remove_abbreviations}
# Because all strings are unnested on the end of sentence period, periods that appear before that moment have to be removed. These include periods in standard abbreviations (i.e. F.B.I.), common titles (Mr./Mrs.), and ellipses (... or . . . ). 

all_works_noabbreviation <- all_works_clean_import %>%
  mutate(cleaned = rm_abbreviation(text, replacement = "abbreviationremoved ")) %>%
  mutate(cleaned = str_replace_all(cleaned, "Mr\\.", "Mr ")) %>%
  mutate(cleaned = str_replace_all(cleaned, "Mrs\\.", "Mrs ")) %>%
  mutate(cleaned = str_replace_all (cleaned, "\\.\\.\\.", " punctellipse ")) %>%
  mutate(cleaned = str_replace_all (cleaned, "\\.\\s\\.\\s\\.\\s", " punctellipse ")) 
```

```{r}
# secondary_works_abbreviations <- secondary_works_clean %>% 
#   mutate(cleaned = rm_abbreviation(text, replacement = "abbreviationremoved")) %>%
#   mutate(cleaned = str_replace_all(cleaned, "Mr\\.", "Mr")) %>%
#   mutate(cleaned = str_replace_all(cleaned, "Mrs\\.", "Mrs")) %>%
#   mutate(cleaned = str_replace_all (cleaned, "\\.\\.\\.", "punctellipse ")) %>%
#   mutate(cleaned = str_replace_all (cleaned, "\\.\\s\\.\\s\\.\\s", "punctellipse ")) 
```

### Secondary Descriptions

```{r create_text_types}
#Work types (short_story or novel) can be created by labeling everything shorter than the shortest novel As I Lay Dying (50k words) as a short story.

all_works_noabbreviation_type <- all_works_noabbreviation %>%
  mutate(work_length = str_count(cleaned, "\\S+")) %>%
  mutate(type = ifelse(work_length > 40000, "novel", "short_story"))

```

```{r corpus_type_ratio}
#This calculates the percentage breakdown of the corpus. All functions have been left verbose for clarity.

novel_short_story_length <- all_works_noabbreviation_type %>%
  group_by(type) %>%
  summarise (work_type_length = sum(work_length)) %>%
  ungroup() %>%
  mutate(percent = work_type_length / sum(work_type_length))

short_story_percentage <- novel_short_story_length %>%
  filter(type == "short_story") %>%
  select(percent)

```

```{r unnest_sentences}
#Create sentences using regex unnest. This works better than unnest_sentences in tidytext library, which drops all columns.

works_length_noabbreviation <- all_works_noabbreviation_type %>%
  group_by(title, date, code, type) %>%
  unnest_regex(sentence, cleaned, "[.?!]") %>%
  mutate(sentence = str_replace(sentence,"”(?=\\s{1})","")) %>% 
  mutate (string_length = str_count(sentence, "\\S+")) %>%
  mutate(ellipse = str_count(sentence, "punctellipse"))  %>%
 
  filter(string_length > 0)
```

```{r}
initial_version_of_sentences <- works_length_noabbreviation
```

```{r}
# secondary_length <- secondary_works_abbreviations %>% 
#                      group_by(title, date) %>%
#   unnest_regex(sentence, cleaned, "[.?!]") %>%
#   
#   mutate (string_length = str_count(sentence, "\\S+")) %>%
#   mutate(ellipse = str_count(sentence, "punctellipse"))  %>%
#   filter(string_length > 0)
```

```{r}
# secondary_overview <- secondary_length %>% 
#                       group_by(title) %>% 
#                       summarise(average_length = mean(string_length))
```

```{r count_pauses}

#Counts the average pauses in a work. This is used to find works with unusually high numbers of a particular type of punctuation.

works_punctuation_abbreviation <- works_length_noabbreviation %>%
  mutate(comma = str_count(sentence, "\\,")) %>%
  mutate(semi_colon = str_count(sentence, "\\;")) %>%
  mutate(dash = str_count(sentence, "[\u2013\u2014]")) %>%
  mutate(colon = str_count(sentence, "\\:")) %>%
  mutate(parenthesis = str_count(sentence, "[\\(\\)]")) 


```

```{r}

```

```{r}
works_punctuation_abbreviation %>%
  filter(string_length<100) %>% 
  ggplot(aes(x = string_length)) +
  geom_histogram(aes(y = ..density..),
                 colour = "black",
                 fill = "white") +
  geom_density(alpha = .2, fill = "#FF6666") +
  ggtitle("String Length Distribution")
```

```{r}
summary(works_punctuation_abbreviation)
```

## Summary Statistics and Exploratory Data Analysis

```{r summary_punctuation}
# Create punctuation table and remove Requiem because the punctuation patterns do not match that of a novel. This table is used continuously downstream.

summary_punctuation <- works_punctuation_abbreviation %>% 
                        group_by(title,date, code,type) %>% 
                        summarise(across(string_length:parenthesis, ~mean(.x))) %>% 
                        # filter(type =='novel') %>% 
                        filter(title!= 'requiem')

```

```{r create_summary_by_type}

# Shows that there are slight differences in short story and novel punctuation distribution, but ultimately not all that interesting. 

summary_by_type <- works_punctuation_abbreviation %>% 
                        group_by(type) %>% 
                        summarise(across(string_length:parenthesis, ~mean(.x)))
```

```{r plot_summary_by_type}

#Again the plot shows fractional differences, but not clear how statistically significant these are.

summary_by_type %>% 
  select(-string_length, -comma) %>% 
  gather(punctuation, value, -type ) %>% 
ggplot(aes(y = value, x = punctuation, fill=type)) +
    geom_bar(stat = "identity", position = "dodge") 
```

```{r summary_by_date}
summary_by_date <- works_punctuation_abbreviation %>% 
                    group_by(date,type) %>% 
                        summarise(across(string_length:parenthesis, ~mean(.x)))
```

```{r plot_summary_by_date}
summary_by_date %>% 
  select(date, type, string_length) %>% 
  
ggplot(aes(y = string_length, x = date, fill=type)) +
    geom_bar(stat = "identity", position = "dodge") 

```

```{r}
punctuation_results <- summary_punctuation %>% 
                        select(title:type)
```

## Sentence length

It does not require computational analysis to understand that Faulkner writes in long sentences. For anyone who has ever taught Faulkner, it is perhaps the most common stumbling block and, indeed, complaint among students. Faulkner experienced this resistance himself during the series of lectures he gave at the University of Virginia, no fewer than 4 participants asked why he wrote with such long sentences (CITE). However, quantifying what readers might mean when they say Faulkner's sentences are long is a bit more complex. On average, at 17 words per sentence Faulkner's sentences are only slightly longer than the average 16 words per sentence for a novel in the Corpus of English Novels, which covers writing from 1881 - 1922 (Ihrmark 79) and longer still than the x words in so and so's study for this period. Nor is the slightly higher than average sentence length a feature of all his works. The *Sound and the Fury* is notoriously complex. Nevertheless, at only 10 words per sentence, it uses fewer words on average than Hemingway's *The Old Man and the Sea*, which 13 words per sentence (Ihrmark 82). Despite this statistical reality, Faulkner's more verbose style would never be confused with Hemingway's trademark concision. In part, this has to do with the distribution of sentence lengths within each text. The *The Sound and the Fury* may have a lower average word count than *The Old Man and the Sea*, but it also contains one sentence that counts over 4,000 words. Clearly, this is less succinct than Hemingway, and probably most other authors as well. These long sentences, it should be noted, are more of a statistical exception than the rule. The normal distribution for this text, and most of Faulkner's other texts is heavily right-skewed. Meaning that for the most part they tend of have "average"-length sentences interspersed with much longer sentences. In the case of the *Sound and the Fury* roughly 90% of the sentences are 17 words or fewer in length. In this text, and most of Faulkner's other texts (60%), the average sentence length is actually lower than the total average across the corpus. More succinctly, just as a small share of sentences in each work gives the impression of long sentences. Likewise, within the corpus of Faulkner's writing there are a minority of works that are above average in sentence length. Lengthy sentences are a local feature not a global phenomenon.

Despite statistical indications to the contrary, lengthy sentences are seen as hallmark feature of Faulkner's writing. In part this may be because some of Faulkner's most canonical works feature long sentences, *Go Down, Moses*, *Absalom, Absalom*, "A Rose for Emily", and "Barn Burning". These works are, in a sense, more Faulknerian than a relatively unknown early text like "Elly". Nevertheless, equally canonical works like *The Sound and the Fury*, *Light in August*, and *As I Lay Dying* do not, on average, stand out as being particularly verbose relative to the the general mass of texts written in English at this particular time. Despite these counter indications, it goes against a firmly established scholarly tradition based on Faulkner's own statements and, indeed, common sense, to argue that lengthy sentences aren't Faulkner's particular metier. Instead, to understand why Faulkner's sentences, for lack of a better word, feel long it is informative to look at their internal structure.

### Internal structure of sentences

```{r}
average_sentence_length <- works_punctuation_abbreviation %>% 
                  group_by(title, code) %>% 
                  summarise(average_string_length=mean(string_length)) 
 
texts_with_long_sentences <- mean(average_sentence_length$average_string_length>mean(average_sentence_length$average_string_length))



sentences_sf <- nrow(works_length_noabbreviation %>% filter(code=="SF"))
long_sentences_sf <-  nrow(works_length_noabbreviation %>% filter(code=="SF") %>% filter(string_length>17))
percent_long <- long_sentences_sf/sentences_sf

  
```

```{r}
summary(works_punctuation_abbreviation %>% 
  filter(code=="SF"))
```

### String Length

#### Distribution Chart

```{r string_length_distrtibution}
summary_punctuation %>% 
ggplot(aes(x=string_length)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("String Length Distribution")


```

#### QQ Plot

```{r string_qqplot}
string_length_shapiro <- shapiro.test(log1p(summary_punctuation$string_length))

ggqqplot(log1p(summary_punctuation$string_length), xlab = "Text", title = paste("String Length Distribution", "Shapiro P value < ", round(string_length_shapiro$p.value,6))
         )
```

#### Rosner Test

```{r string_distribution}
string_length_outlier <- boxplot.stats(log1p(summary_punctuation$string_length))$out
string_length_outlier_rownumbers <- which(log1p(summary_punctuation$string_length) %in% c(string_length_outlier))

string_length_test <- rosnerTest(log1p(summary_punctuation$string_length),
  k = length(string_length_outlier_rownumbers)
)
string_length_outlier_obs <-  string_length_test$all.stats %>% 
                       filter(Outlier ==TRUE) %>% 
                       select(Obs.Num)

#Write value to punctuation results

string_length_outlier_result <- summary_punctuation[string_length_outlier_obs$Obs.Num,] %>% 
                                ungroup() %>% 
                                select(title, string_length)
```

```{r string_distribution}

```

#### Adjusted Boxplot

```{r}
string_length_adjusted_boxplot <-
  boxB(summary_punctuation$string_length, method = "adjbox")
string_length_boxplot_outlier_result <-
  summary_punctuation[string_length_adjusted_boxplot$outliers, ] %>%
  ungroup() %>%
  select(title, string_length)
```

```{r}


```

### Ellipse

#### Distribution

```{r ellipse_distribution}
summary_punctuation %>%
  ggplot(aes(x = ellipse)) +
  geom_histogram(aes(y = ..density..),
                 colour = "black",
                 fill = "white") +
  geom_density(alpha = .2, fill = "#FF6666") +
  ggtitle("Ellipse Distribution")
```

#### QQ Plot

```{r ellipse_distribution}
ellipse_shapiro <- shapiro.test(1/summary_punctuation$ellipse)

ggqqplot(
  1/summary_punctuation$ellipse,
  xlab = "Text",
  title = paste(
    "Ellipse Distribution",
    "Shapiro P value < ",
    round(ellipse_shapiro$p.value, 6)
  )
)
```

#### Rosner Test

```{r ellipse_rosner}
#I tried working with the inverse here, but got mixed results. It pulls out all the low observations as unusual instead of simply seeing them as zero. It looks like both with the lognormal and the regular the top still clusters as outliers. 

ellipse_inverse <- summary_punctuation %>% 
                  mutate(ellipse = ellipse^-1)



ellipse_no_zero <- summary_punctuation %>% 
                    mutate(ellipse = 1/ellipse) %>% 
                    mutate(ellipse = ifelse(ellipse == "Inf", 0,ellipse))
ellipse_outlier <-
  boxplot.stats(ellipse_no_zero$ellipse)$out
ellipse_outlier_rownumbers <-
  which((ellipse_no_zero$ellipse) %in% c(ellipse_outlier))

ellipse_test <- rosnerTest((ellipse_no_zero$ellipse),
                           k = length(ellipse_outlier_rownumbers))
ellipse_outlier_obs <-  ellipse_test$all.stats %>%
  filter(Outlier == TRUE) %>%
  select(Obs.Num)

ellipse_outlier_result <-
  ellipse_no_zero[ellipse_outlier_obs$Obs.Num, ] %>%
  ungroup() %>%
  select(title, ellipse)


```

#### Adjusted Boxplot

```{r ellipse_adjusted_box}
ellipse_adjusted_boxplot <-
  boxB(ellipse_no_zero$ellipse, method = "adjbox")

ellipse_boxplot_outlier_result <-
  summary_punctuation[ellipse_adjusted_boxplot$outliers, ] %>%
  ungroup() %>%
  select(title, ellipse)


```



```{r}

#Simple test to verify the most numerous result. As suspected, the top 5 are somewhat unexpected.

summary_punctuation %>% 
  ungroup() %>% 
  slice_max(ellipse,n=10) %>% 
  
  ggplot( aes(x=title, y=ellipse)) + 
  geom_bar(stat = "identity")


```



### Comma

#### Distribution

```{r comma_distribution}
summary_punctuation %>%
  ggplot(aes(x = comma)) +
  geom_histogram(aes(y = ..density..),
                 colour = "black",
                 fill = "white") +
  geom_density(alpha = .2, fill = "#FF6666") +
  ggtitle("comma Distribution")
```

#### QQ Plot

```{r}
comma_shapiro <- shapiro.test(summary_punctuation$ellipse)

ggqqplot(summary_punctuation$comma, xlab = "Text", title = paste("Comma Distribution", "Shapiro P value < ", round(comma_shapiro$p.value,6))
         )
```

#### Rosner Test

```{r}
comma_outlier <- boxplot.stats(summary_punctuation$comma)$out
comma_outlier_rownumbers <-
  which(summary_punctuation$comma %in% c(comma_outlier))

comma_test <- rosnerTest(summary_punctuation$comma,
                         k = length(comma_outlier_rownumbers))

comma_outlier_obs <-  comma_test$all.stats %>%
  filter(Outlier == TRUE) %>%
  select(Obs.Num)

comma_outlier_result <- summary_punctuation[comma_outlier_obs$Obs.Num,] %>% 
                                ungroup() %>% 
                                select(title, comma)


```

#### Adjusted Boxplot

```{r}
comma_adjusted_boxplot <-
  boxB(summary_punctuation$comma, method = "adjbox")

comma_boxplot_outlier_result <-
  summary_punctuation[comma_adjusted_boxplot$outliers, ] %>%
  ungroup() %>%
  select(title, comma)


```

### Semi-Colon

#### Distribution

```{r semi_colon_distribution}
summary_punctuation %>%
  ggplot(aes(x = semi_colon)) +
  geom_histogram(aes(y = ..density..),
                 colour = "black",
                 fill = "white") +
  geom_density(alpha = .2, fill = "#FF6666") +
  ggtitle("Semi-Colon Distribution")
  
```

#### QQ Plot

```{r semi_colon_qq}
semi_colon_shapiro <- shapiro.test(summary_punctuation$semi_colon)

ggqqplot(
  log1p(summary_punctuation$semi_colon),
  xlab = "Text",
  title = paste(
    "Semi-Colon's Per Word",
    "Shapiro P value < ",
    round(semi_colon_shapiro$p.value, 6)
  )
)
```

#### Rosner Test

```{r semi_colon_qq}
semi_colon_outlier <-
  boxplot.stats(log1p(summary_punctuation$semi_colon))$out
semi_colon_outlier_rownumbers <-
  which(log1p(summary_punctuation$semi_colon) %in% c(semi_colon_outlier))


semi_colon_test <- rosnerTest(log1p(summary_punctuation$semi_colon),
                              k = length(semi_colon_outlier_rownumbers))
semi_colon_outlier_obs <-  semi_colon_test$all.stats %>%
  filter(Outlier == TRUE) %>%
  select(Obs.Num)

semi_colon_outlier_result <-
  summary_punctuation[semi_colon_outlier_obs$Obs.Num, ] %>%
  ungroup() %>%
  select(title, semi_colon)



```

#### Adjusted Box Plot

```{r}
semi_colon_adjusted_boxplot <- boxB(summary_punctuation$semi_colon, method="adjbox")

semi_colon_boxplot_outlier_result <- summary_punctuation[semi_colon_adjusted_boxplot$outliers,] %>% 
                                ungroup() %>% 
                                select(title, semi_colon)


```

### Dash

#### Distribution

```{r}
summary_punctuation %>% 

ggplot(aes(x=dash)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
   ggtitle("Dash Distribution")
```

#### Dash QQ Plot

```{r}
dash_shapiro <- shapiro.test(log1p(summary_punctuation$dash))

ggqqplot(log1p(summary_punctuation$dash), xlab = "Text", title = paste("Dashes Per Word", "Shapiro P value < ", round(dash_shapiro$p.value,6))
         )
```

#### Rosner Test

```{r}
dash_outlier <- boxplot.stats(log1p(summary_punctuation$dash))$out
dash_outlier_rownumbers <- which(log1p(summary_punctuation$dash) %in% c(dash_outlier))

dash_test <- rosnerTest(log1p(summary_punctuation$dash),
  k = length(dash_outlier_rownumbers)
)
dash_outlier_obs <-  dash_test$all.stats %>% 
                       filter(Outlier ==TRUE) %>% 
                       select(Obs.Num)

dash_outlier_result <- summary_punctuation[dash_outlier_obs$Obs.Num,] %>% 
                                ungroup() %>% 
                                select(title, dash)

```

#### Adjusted Boxplot

```{r}
dash_adjusted_boxplot <- boxB(summary_punctuation$dash, method="adjbox")

dash_boxplot_outlier_result <- summary_punctuation[dash_adjusted_boxplot$outliers,] %>% 
                                ungroup() %>% 
                                select(title, dash)




```

### Colon

#### Distribution

```{r}
summary_punctuation %>% 
ggplot(aes(x = colon)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("Colon Distribution")

```

#### QQ Plot

```{r}
colon_shapiro <- shapiro.test(1/summary_punctuation$colon)

ggqqplot(log1p(summary_punctuation$colon), xlab = "Text", title = paste("Colons Per Word", "Shapiro P value < ", round(colon_shapiro$p.value,6))
         )
```

#### Rosner Test

```{r}
colon_outlier <- boxplot.stats(log1p(summary_punctuation$colon))$out
colon_outlier_rownumbers <- which(log1p(summary_punctuation$colon) %in% c(colon_outlier))

colon_test <- rosnerTest(log1p(summary_punctuation$colon),
  k = length(colon_outlier_rownumbers)
)
colon_outlier_obs <-  colon_test$all.stats %>% 
                       filter(Outlier ==TRUE) %>% 
                       select(Obs.Num)

colon_outlier_result <- summary_punctuation[colon_outlier_obs$Obs.Num,] %>% 
                                ungroup() %>% 
                                select(title, colon)


                       
```

#### Colon Adjusted Box Plot

```{r}
colon_adjusted_boxplot <- boxB(summary_punctuation$colon, method="adjbox")

colon_boxplot_outlier_result <- summary_punctuation[colon_adjusted_boxplot$outliers,] %>% 
                                ungroup() %>% 
                                select(title, colon)


```

### Parenthesis

#### Distribution

```{r}
summary_punctuation %>% 
ggplot(aes(x = parenthesis)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("Parenthesis Distribution")
```

#### QQ Plot

```{r}
parenthesis_shapiro <- shapiro.test(log1p(summary_punctuation$parenthesis))

ggqqplot(log1p(summary_punctuation$parenthesis), xlab = "Text", title = paste("parenthesiss Per Word", "Shapiro P value < ", round(parenthesis_shapiro$p.value,6))
         )
```

#### Rosner Test

```{r}
parenthesis_outlier <- boxplot.stats(log1p(summary_punctuation$parenthesis))$out
parenthesis_outlier_rownumbers <- which(log1p(summary_punctuation$parenthesis) %in% c(parenthesis_outlier))

parenthesis_test <- rosnerTest(log1p(summary_punctuation$parenthesis),
  k = length(parenthesis_outlier_rownumbers)
)
parenthesis_outlier_obs <-  parenthesis_test$all.stats %>% 
                       filter(Outlier ==TRUE) %>% 
                       select(Obs.Num)

parenthesis_outlier_result <- summary_punctuation[parenthesis_outlier_obs$Obs.Num,] %>% 
                                ungroup() %>% 
                                select(title, parenthesis)


                       
```

#### Adjusted Box Plot

```{r}
parenthesis_adjusted_boxplot <- boxB(summary_punctuation$parenthesis, method="adjbox")

parenthesis_boxplot_outlier_result <- summary_punctuation[parenthesis_adjusted_boxplot$outliers,] %>% 
                                ungroup() %>% 
                                select(title, parenthesis)


```

### All Outliers

```{r}
# Create punctuation result
punctuation_results_full_table <- NULL
punctuation_results_full_table <-  punctuation_results %>% 
  left_join(string_length_outlier_result, by = join_by(title)) %>% 
  rename(string_length_rosner = string_length) %>% 
  left_join(string_length_boxplot_outlier_result, by = join_by(title)) %>%
  rename(string_length_boxplot = string_length) %>% 
  left_join(ellipse_outlier_result, by = join_by(title)) %>%
  rename(ellipse_rosner = ellipse) %>% 
  left_join(ellipse_boxplot_outlier_result, by = join_by(title)) %>% 
  rename(ellipse_boxplot = ellipse) %>% 
  left_join(comma_outlier_result, by = join_by(title)) %>%
  rename(comma_rosner = comma) %>% 
  left_join(comma_boxplot_outlier_result, by = join_by(title)) %>%
  rename(comma_boxplot = comma) %>% 
  left_join(semi_colon_outlier_result, by = join_by(title)) %>%
  rename(semi_colon_rosner = semi_colon) %>% 
  left_join(semi_colon_boxplot_outlier_result, by = join_by(title)) %>% 
  rename(semi_colon_boxplot = semi_colon) %>% 
  left_join(dash_outlier_result, by = join_by(title)) %>% 
  rename(dash_rosner = dash) %>% 
  left_join(dash_boxplot_outlier_result, by = join_by(title)) %>% 
  rename(dash_boxplot = dash) %>% 
  left_join(colon_outlier_result, by = join_by(title)) %>% 
  rename(colon_rosner = colon) %>% 
  left_join(colon_boxplot_outlier_result, by = join_by(title)) %>% 
  rename(colon_boxplot = colon) %>% 
  left_join(parenthesis_outlier_result, by = join_by(title)) %>% 
  rename(parenthesis_rosner = parenthesis) %>% 
  left_join(parenthesis_boxplot_outlier_result, by = join_by(title)) %>% 
  rename(parenthesis_boxplot = parenthesis)


```

```{r}
all_outliers <- punctuation_results_full_table %>% 
  rowwise() %>% 
  mutate(outlier = sum(c_across(where(is.numeric)), na.rm = T)) %>% 
  mutate(outlier = ifelse(outlier>0,TRUE,FALSE)) %>% 
  filter(outlier == TRUE)  %>% 
  mutate_at(c(8,11), as.numeric)

```

```{r}
all_outliers_count <- all_outliers %>% 
    select(5:18) %>%  
    is.na %>% 
    `!` %>% 
    rowSums

all_outliers_count <- all_outliers %>% 
  add_column(all_outliers_count)
    
```

```{r}

```

```{r}
#| include: false

# pauses <- summary_punctuation %>% 
#           ungroup() %>% 
#           mutate(across(c(ellipse:parenthesis), ~log1p(.x))) %>% 
#           select(ellipse:parenthesis)
# 

```

```{r}
# ggplot(gather(pauses),  aes(x=key, y=value)) + geom_boxplot(aes(fill=key))
```

## Extending the Sentences: Pauses and Such

The preceding discussion about sentence length raises an obvious question: What is a long sentence? Is it any sentence above the average in a corpus? If so, how much longer does it need to be to be consider long and not simply longer than usual? At what point, does a string of words grammatically cease to be sentence? James Joyce famously wrote "the longest sentence" in English in the Penelope chapter of *Ulysses*, but this sentence is not long in the same way a sentence in Faulkner is long. While there is certainly semantic coherence underlying Joyce's string of words, the lack of mid-sentence punctuation makes it grammatically incorrect if not also syntacitacticly incoherent. This was rather point, of course, but it is an important distinction to make. Faulkner not only creates long strings of words, but also coordinates and subordinates his clauses through punctuation. Thus, whereas Joyce saw the intervention of punctuation as an infelicity to the ineluctable flow of meaning making through language in the Penelope episode, the sheer profusion of punctuation in Faulkner speaks to the endless way in which meaning splits, divaricates, rejoins and ends in an arbitrary terminus only to continue again. To use two spatial metaphors, The Penelope chapter is a stream, Faulkner's writing is a watershed.

To understand how Faulkner lengthens his sentences it is useful to look at mid-sentence punctuation. While sentence length can be calculated by measuring the distance between the first capital letter and full-stop, it is possible to get a sense of how the sentence diverts, tarries, and pauses by looking at mid-sentence punctuation like: colons, ellipses, and parenthesis. Faulkner is not merely making sentences very long, but using punctuation to stretch out the definition of a sentence, and, in turn, call attention to the punctuation itself. The finality of the period is replaced with the indeterminacy of the unfinished sentence. What is telling about Faulkner's writing is that he does not simply write long sentences throughout his career, but experiments with different punctuation configurations to make this possible. Arguably, any number of texts serve as good examples, but there are three salient texts that deviate from Faulkner's own writing statistically: "Dry September", Absalom, and Intruder in the Dust.

To determine which of Faulkner's texts have punctuation at variance with the corpus more generally,a statistical model of Faulkner's short stories and novels was created. The play, *A Requiem for a Nun* was excluded from this analysis, punctuation patterns distorted the average. With this smaller corpus, the texts were broken down into sentences and each mid-sentence punctuation mark was counted. Intuitively, sentence length and mid-sentence punctuation are positively correlated. As sentences get longer, there is a higher likelihood of finding mid-sentence punctuation. There are some functional limits to this correlation, however. Modern punctuation tends to function as a syntactical system that regulates the number of certain punctuation marks that can be used (@Shou 198). This is particularly true of colons, semi-colons, and parenthesis. For example, one line in *The Mansion* (1959) has 9 semi-colons and is over 400 words long. It pushes the limits as to how many independent clauses a reader can reasonably be expected to understand in the same ostensibly coherent thought delimited by a period.

By taking the averages of the number of mid-sentence punctuation marks across all texts, it is possible to look at their distribution. In general, these follow a right-skewed distribution; a bell curve with a long tail. Syntactically, this makes sense. If a sentence contains one colon, it is far less likely that it would also contain a second colon. This likelihood tends to decrease precipitously, hence the very long tail in the distribution. Determining when a value on that tail becomes an "outlier" is a matter of interpretation, and there is no singular test that can determine this (CITE). As the distance between the mean and the highest value was quite far apart, the skewness of the distribution was corrected for by either taking the inverse or the log of each of the values. Each result was then fitted onto a quantile-quantile plot to test for normal distribution. Using these distributions, two different tests were run to test for outliers, Rosner's and Adjusted box plot developed by SO AND SO. The tests revealed slightly different outliers, but on the whole they suggest an inconsistent use of mid-sentence punctuation across the corpus. That is, Faulkner uses some mid-sentence punctuation in some texts more than in others. This indicates some measure of experimentation with how he is creating long sentences. If he were using only one particular technique, for example, like a string of independent clauses separated by a semi-colon as in *The Mansion*, the other mid-sentence punctuation should remain follow similar distributions. This is not the case, and indicates variance in punctuation techniques across his career.

Three texts provide useful insights. *Absalom, Absalom*, and *Intruder in the Dust*, and "Dry September" all stood out for being outliers. As one of Faulkner's most challenging texts, it is unsurprising that the punctation patterns in *Absalom, Absalom!* diverge from the corpus. Averaging around 50 words per sentence, the text is marked by a heavy use of parentheses. Thus, a sentence will be interupted by parenthetical statements, which in themselves are not usually sentences, leading to sentences of extraordinary length. In one sentence, he uses 9 parenthetical statements as a running commentary on the main sentence (AA 124-127). Some of these parentheticals are themselves several lines. All of this is complicated by the fact that Chapter 6 is itself entirely surrounded by parentheses. As a result, the parentheses in this sentence are actually nested within surrounding parentheses. This nesting does continues throughout the Chapter and at some point the the word Quentin is nested 4 levels deep. That is, as an aside to an aside to an aside. Not only are Faulkner's sentences very long in Absalom, Absalom! they are also deep. Indeed, the entire novel counts over 1,200 parentheses.

In a slightly different fashion, Faulkner instrumentalizes colons in *Intruder in the Dust* to blank. To give some extreme examples, one sentence in *Intruder in the Dust* is over 1600 words long and features 18 colons. The sentence deploys colons as a clausal adjunct. Nunberg labels this phenomenon colon-expansion, whereby the content following the colon expands or elaborates on the preceding clause (Nunberg 30). Importantly, he points out that there are only two contraints to a colon expansion. First, there is a semantic limit to the extent to which something can be elaborated, and, two, colon-expansions cannot themselves contain other colon-expansions (Nunberg 30-31). Faulkner violates the former by dint of the latter. Each sentence dialates in meaning as he adds on more clausal adjuncts and elaborations. While it is impractical to quote the sentence in its entirety, the opening sequence does a good job of capturing this effect: "Because he was free: in bed: in the cool familiar room...(34)". With each colon more details about the narrative situation are revealed. The choice for concatenating clauses with a colon and not simply writing new sentences, appears to be similar to the depth effect created through parentheticals in *Absalom, Absalom!*. With each new colon the reading is tasked with processing a new piece of information that falls within the scope of one sentence. The narrative does not move forward but instead moves deeper. 

It does not require inferential statistics to understand that the punctuation in AA and Intruder are unusual. Yet, one text that jumped out of the statistical model that appears odd is "Dry September." This is a highly canonical Faulkner story and frequently features in literature anthologies because it is very teachable. Compared to intruder and absalom, this is easier to read.  is more an unusual example, it has more ellipses per sentence than expected for a work of that length, but uses relatively few words per sentence. This contradiction speaks to the stories central thematic: the lynching of Will Mayes, an innocent Black man, over town rumors about him assaulting a White woman. The ellipses are exclusively contained to when characters are speaking and omit important details. It is a way for Faulkner to continue the sentence through other means. These few examples do not touch upon the challenge SF represents. After all, some of the long sentences in SF are actually clauses that start with capital letter, but are not closed by a period. It raises the question as to whether these are sentences or clauses.

These short examples demonstrate that Faulkner's texts are marked by long sentences, but the means of achieving this length is not consistent and it is something with which he experiments with throughout this career.

Onto the next section

```{r}

mosquito_sentences <- works_punctuation_abbreviation %>% 
  filter(title == "mosquitoes")

mosquito_sentence_ellipse <- mean(mosquito_sentences$ellipse>0)
  
  
```

In short, it provided a window into what is a "normal" number of punctuation marks per word in all of Faulkner's texts. Based on this data it is possible to infer outliers, or data that appears irregular. This procedure is highly interpretive. First, since the data is exclusively about Faulkner's texts, it is hard to know how non-normal his punctuation is relative to other authors. Second, the corpus of novels and collected and uncollected stories itself is heterogeneous. The most salient example is *Requiem for a Nun*, which is written as a play and therefore has a higher number of parenthesis consonant with that form. Likewise, a substantial percentage `r round(short_story_percentage$percent, 0)`% of the texts are short stories. These stories have a slightly shorter average sentence length of `r round(summary_by_type$string_length[2],0)` words versus `r round(summary_by_type$string_length[1], 0)` words in the novels. Including or excluding Requiem for a Nun and the short stories creates a slighlty different mean across the board, which, in turn, sets a different baseline as to what constitutes Faulkner's "normal" writing. Since there are convincing arguments to be made for both scenarios, both were tested for outliers.

Generally, the normal distribution for punctuation marks in Faulkner's texts is right-skewed; the curve has a long tail to the right. Intuitively, this makes sense. Unlike grades which follow a normal, bell-shaped distribution, punctuation marks in a sentence do not have a defined upper limit. It is therefore possible to have some texts with a substantially higher frequency of a particular kind of punctuation marks per word. In the same way that it is possible for some people to have a much higher income than the mean income. The common procedure for dealing with right skewed data is to take the natural log and test for a normal distribution. This essentially reduces the distance between x values by scaling them. The resulting data can then be tested for normalcy. This was done statistically with the Shapiro-Wilk normality test and visually with a qq chart. Once normalcy has been confirmed it is possible to test for outliers. While there are plenty of statistical tests for outliers, this is to some extent a subjective practice. Each test has some underlyinig assumptions. That said, using the Rosner test on all of the punctuations revealed three texts that were statistically unusual: Mosquitos had significantly more parenthesis per word than any of the other texts and Absalom, Absalom! had signficantly more parenthesis than other texts. Intruder in the Dust had significantly more colons than all the other texts, but this difference only showed itself when all of the texts were included.

NOTE: Punctuation like other linguistic phenomenon follows logrithmic regression or the power law like zipf's law. While so and so attribute this to a least-effort hypothesis, instead it is far more related to the unity of thought. Less punctuation is more directive. QUote from shou

Since punctuation follows discernible rules, it makes sense that he distribution of punctuation marks is right-skewed; it has a long tail to the right. That is, it is very likely that given sentence length x that there are n-number of mid-sentence punctuation marks, it is far less likely that there are more than n and impossible for there to be negative puncuation marks. The odds of there being more than n drop precipitiously because that would mean violating the basic rules of grammar. It is these violations that are interesting. There are several statistical methods for detecting these rule breakers. They all rely on some measure of interpretation. In the hard sciences an outlier is usually described as something that inordinately skews the data a broken pipette or someone who did not properly understand the questionarie. The rules of grammar are different.

With the exception of commas the distributions are very narrow with a long, right-skewed tail. This makes a lot of sense when we consider the basic structure of grammar. It is very possible to have multiple commas in one sentence, it is much less likely to have multiple colons or multiple semi-colons. Likewise, sentences generally only have one set of parenthesis.

## Import Demographics

```{r}
#demographic data goes here. Read in file then come up with a measure of class. % upper class men present.

demographic <- read_csv("processed_data/dy_database_flattened_2023_6_29.csv")
```

```{r}
short_demographic <- demographic %>% 
                      select(SourceTextTitle,SourceTextCode,PageNumber,Nid,PresentMentioned,Race,Gender,Class,Rank) %>%  
                      filter(PresentMentioned=="Present")
                      
```

```{r}
race_demographic <- short_demographic %>% 
  group_by(SourceTextCode,Nid) %>% 
                      pivot_wider(id_cols = c(SourceTextCode,PageNumber,Nid), names_from = Race, values_from = Gender, values_fn = list(Gender=length), names_prefix = "race_")  
          
  



```

```{r}
race_demographic_summary <- race_demographic %>% 
                            group_by(SourceTextCode) %>% 
                              summarise(across(starts_with("race_"), ~ sum(.x, na.rm = TRUE))) %>% 
                              group_by(SourceTextCode) %>% 
  mutate(total = sum(c_across(starts_with("race_")), na.rm = TRUE)) %>% 
    mutate(white_percent = race_White/total)

```

```{r}
class_demographic <- short_demographic %>% 
  group_by(SourceTextCode,Nid) %>% 
                      pivot_wider(id_cols = c(SourceTextCode,PageNumber,Nid), names_from = Class, values_from = Gender, values_fn = list(Gender=length), names_prefix = "class_") %>% 
  rename_with(~ tolower(gsub(" ", "_", .x, fixed = TRUE)))
            
```

```{r}
class_demographic_summary <- class_demographic %>% 
                            group_by(sourcetextcode) %>% 
                              summarise(across(starts_with("class_"), ~ sum(.x, na.rm = TRUE))) %>% 
                              group_by(sourcetextcode) %>% 
  mutate(total = sum(c_across(starts_with("class_")), na.rm = TRUE)) %>% 
    mutate(upper_percent = class_upper_class/total)
```

```{r}
gender_demographic <- short_demographic %>% 
  group_by(SourceTextCode,Nid) %>% 
                      pivot_wider(id_cols = c(SourceTextCode,PageNumber,Nid), names_from = Gender, values_from = Class, values_fn = list(Class=length), names_prefix = "gender_") %>% 
  rename_with(~ tolower(gsub(" ", "_", .x, fixed = TRUE)))

```

```{r}
gender_demographic_summary <- gender_demographic %>% 
                            group_by(sourcetextcode) %>% 
                              summarise(across(starts_with("gender_"), ~ sum(.x, na.rm = TRUE))) %>% 
                              group_by(sourcetextcode) %>% 
  mutate(total = sum(c_across(starts_with("gender_")), na.rm = TRUE)) %>% 
    mutate(male_percent = gender_male/total)
```

```{r}
#| include: false

demographics_punctuation <- summary_punctuation %>% 
                            inner_join(select(race_demographic_summary, SourceTextCode, white_percent),   by = join_by(code==SourceTextCode)) %>% 
                           inner_join(select(class_demographic_summary, sourcetextcode, upper_percent),   by = join_by(code==sourcetextcode)) %>% 
                            inner_join(select(gender_demographic_summary, sourcetextcode, male_percent),   by = join_by(code==sourcetextcode))
```

```{r}
#correlations

correlations_string_length <- demographics_punctuation %>% 
                              filter(type=="novel") %>% 
                               ungroup() %>% 
                               summarise(across(where((is.numeric)),~ cor(.x,string_length, method="pearson")))


```

```{r}
demographics_punctuation %>% 
ggplot(aes(date, string_length)) +
  geom_line()


```

```{r}
raceclassgender_demographic <- short_demographic %>% 
                               mutate(race_class_gender = paste(Race,Class,Gender,sep="_")) %>% 
                              mutate(race_class_gender = tolower(str_replace_all(race_class_gender," ","_"))) %>% 
                               group_by(SourceTextCode,Nid) %>% 
                      pivot_wider(id_cols = c(SourceTextCode,PageNumber,Nid), names_from = race_class_gender, values_from = Gender, values_fn = list(Gender=length), names_prefix = "all_") %>% 
  rename_with(~ tolower(gsub(" ", "_", .x, fixed = TRUE))) %>% 
  ungroup()
                              
```

```{r}

raceclassgender_demographic_summary <- raceclassgender_demographic %>% 
                                        group_by(sourcetextcode) %>% 
                                        summarise(across(starts_with("all_"), ~ sum(.x, na.rm = TRUE))) %>% 
  group_by(sourcetextcode) %>% 
  mutate(total = sum(c_across(starts_with("all_")), na.rm = TRUE)) %>% 
  mutate(across(where(is.numeric),~ ./total), .names = "{.col}_percent")

```

```{r}
demographics_punctuation_raceclassgender <- summary_punctuation %>% 
                                              inner_join(raceclassgender_demographic_summary,join_by(code==sourcetextcode))
            
```

```{r}
correlations_string_length_rcg <- demographics_punctuation_raceclassgender %>% 
                               filter(type=="novel") %>% 
                              ungroup() %>% 
                              summarise(across(where((is.numeric)),~ cor(.x,string_length, method="pearson"))) %>% 
                              pivot_longer(starts_with("all_"), names_to = "types") %>% 
                              filter(value>.2 | value < -.2)
```

```{r}
key_demographics <- demographics_punctuation_raceclassgender %>% 
                  filter(type=="novel") %>% 
                    select(string_length, any_of(correlations_string_length_rcg$types))

```

### Notes

Consider the long sentence and the flow of history. Who gets to be part of that history? How do we suture folks back into their histories. Who is embedded in these long sentences? Is it Quentin? Is it Stevens?

I can correlate sentence length to character demographics per text weighted frequency of character demographics. That is to say, does the sentence length increase when there are more upper class white characters?

I have a pretty decently cleaned up version of the numbers. Should go through the sentences again to see if there's any major regex errors.

There do not appear to be any strong correlations between sentence length and the racial composition of the character. This effect might appear stronger when combined.

No correlations with race class gender either

"Afternoon of a cow" was published in 1947 nearly 40 years after Faulkner wrote it. Some of the texts are not always published when written.
